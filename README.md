# ü§ñ Hugging Face Educational Models - IA para Educa√ß√£o

## üéØ Objetivo de Aprendizado
Projeto desenvolvido para estudar **Machine Learning** e **NLP (Natural Language Processing)** com **Hugging Face Transformers**, implementando modelos de IA para sumariza√ß√£o de textos, Q&A e gera√ß√£o de perguntas educacionais.

## üõ†Ô∏è Tecnologias Utilizadas
- **Framework:** Hugging Face Transformers
- **Linguagem:** Python 3.7+
- **ML/DL:** PyTorch, TensorFlow
- **NLP:** BERT, T5, GPT models
- **Modelos:** Pre-trained transformers
- **Conceitos estudados:**
  - Natural Language Processing
  - Transfer Learning
  - Transformer architecture
  - Text summarization
  - Question answering
  - Text generation

## üöÄ Demonstra√ß√£o
```python
# Sumariza√ß√£o de textos
from transformers import pipeline

def summarize_text(text):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
    return summary[0]['summary_text']

# Question Answering
def answer_question(context, question):
    qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
    result = qa_pipeline(question=question, context=context)
    return result['answer']

# Gera√ß√£o de perguntas
def generate_questions(text):
    question_generator = pipeline("text2text-generation", 
                                 model="mrm8488/t5-base-finetuned-question-generation-ap")
    questions = question_generator(text)
    return questions
```

## üí° Principais Aprendizados

### üß† Natural Language Processing
- **Tokeniza√ß√£o:** Processamento de texto para modelos
- **Embeddings:** Representa√ß√£o vetorial de palavras
- **Attention Mechanism:** Como modelos focam em partes relevantes
- **Transfer Learning:** Uso de modelos pr√©-treinados

### ü§ñ Modelos Transformer
- **BERT:** Bidirectional Encoder Representations
- **T5:** Text-to-Text Transfer Transformer
- **BART:** Denoising Autoencoder
- **DistilBERT:** Vers√£o otimizada do BERT

### üìö Aplica√ß√µes Educacionais
- **Sumariza√ß√£o:** Resumos autom√°ticos de conte√∫do
- **Q&A Systems:** Sistemas de perguntas e respostas
- **Question Generation:** Cria√ß√£o autom√°tica de quest√µes
- **Content Analysis:** An√°lise de textos educacionais

## üß† Conceitos T√©cnicos Estudados

### 1. **Text Summarization**
```python
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

class TextSummarizer:
    def __init__(self, model_name="facebook/bart-large-cnn"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        self.summarizer = pipeline("summarization", 
                                  model=self.model, 
                                  tokenizer=self.tokenizer)
    
    def summarize(self, text, max_length=130, min_length=30):
        # Verificar tamanho do texto
        if len(text.split()) < 50:
            return "Texto muito curto para sumariza√ß√£o"
        
        summary = self.summarizer(text, 
                                max_length=max_length, 
                                min_length=min_length, 
                                do_sample=False)
        return summary[0]['summary_text']
```

### 2. **Question Answering System**
```python
class QuestionAnsweringSystem:
    def __init__(self, model_name="distilbert-base-cased-distilled-squad"):
        self.qa_pipeline = pipeline("question-answering", model=model_name)
    
    def answer(self, context, question):
        result = self.qa_pipeline(question=question, context=context)
        
        return {
            'answer': result['answer'],
            'confidence': result['score'],
            'start': result['start'],
            'end': result['end']
        }
    
    def batch_answer(self, context, questions):
        answers = []
        for question in questions:
            answer = self.answer(context, question)
            answers.append({
                'question': question,
                'answer': answer['answer'],
                'confidence': answer['confidence']
            })
        return answers
```

### 3. **Question Generator**
```python
class QuestionGenerator:
    def __init__(self, model_name="mrm8488/t5-base-finetuned-question-generation-ap"):
        self.generator = pipeline("text2text-generation", model=model_name)
    
    def generate_questions(self, text, num_questions=5):
        # Preparar texto para o modelo T5
        input_text = f"generate questions: {text}"
        
        questions = self.generator(
            input_text,
            max_length=64,
            num_return_sequences=num_questions,
            temperature=0.7,
            do_sample=True
        )
        
        return [q['generated_text'] for q in questions]
    
    def generate_with_answers(self, text):
        questions = self.generate_questions(text)
        qa_system = QuestionAnsweringSystem()
        
        qa_pairs = []
        for question in questions:
            answer = qa_system.answer(text, question)
            qa_pairs.append({
                'question': question,
                'answer': answer['answer'],
                'confidence': answer['confidence']
            })
        
        return qa_pairs
```

## üìÅ Estrutura do Projeto
```
huggingface-edu-models/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ summarizer.py           # Sumariza√ß√£o de textos
‚îÇ   ‚îú‚îÄ‚îÄ question_answering.py   # Sistema de Q&A
‚îÇ   ‚îú‚îÄ‚îÄ question_generator.py   # Gera√ß√£o de perguntas
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Classes dos modelos
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Utilit√°rios
‚îú‚îÄ‚îÄ examples/                   # Exemplos de uso
‚îú‚îÄ‚îÄ data/                       # Dados de exemplo
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias
‚îî‚îÄ‚îÄ notebooks/                  # Jupyter notebooks
```

## üîß Como Executar

### Instala√ß√£o
```bash
# Clone o reposit√≥rio
git clone <repo-url>
cd huggingface-edu-models

# Instale depend√™ncias
pip install -r requirements.txt

# Ou com conda
conda env create -f environment.yml
conda activate huggingface-edu
```

### Uso B√°sico
```python
# Sumariza√ß√£o
from src.summarizer import TextSummarizer

summarizer = TextSummarizer()
text = "Texto longo para ser sumarizado..."
summary = summarizer.summarize(text)
print(summary)

# Question Answering
from src.question_answering import QuestionAnsweringSystem

qa = QuestionAnsweringSystem()
context = "Contexto com informa√ß√µes..."
question = "Qual √© a resposta?"
answer = qa.answer(context, question)
print(answer)

# Gera√ß√£o de Perguntas
from src.question_generator import QuestionGenerator

qg = QuestionGenerator()
questions = qg.generate_questions(text)
print(questions)
```

## üéØ Aplica√ß√µes Educacionais

### 1. **Sistema de Estudo Automatizado**
- Sumariza√ß√£o de livros did√°ticos
- Gera√ß√£o de quest√µes para revis√£o
- Sistema de Q&A para d√∫vidas

### 2. **Cria√ß√£o de Conte√∫do**
- Gera√ß√£o autom√°tica de exerc√≠cios
- Resumos de aulas e palestras
- Cria√ß√£o de flashcards

### 3. **An√°lise de Texto**
- Extra√ß√£o de conceitos principais
- Identifica√ß√£o de t√≥picos importantes
- Avalia√ß√£o de compreens√£o

## üöß Desafios Enfrentados
1. **Model Selection:** Escolher modelos adequados para cada tarefa
2. **Performance:** Otimizar velocidade de infer√™ncia
3. **Memory Management:** Gerenciar uso de GPU/CPU
4. **Text Preprocessing:** Preparar textos para os modelos
5. **Quality Control:** Avaliar qualidade das sa√≠das
6. **Portuguese Language:** Adaptar para textos em portugu√™s

## üìö Recursos Utilizados
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Papers With Code](https://paperswithcode.com/) - State-of-the-art models
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer paper
- [BERT Paper](https://arxiv.org/abs/1810.04805) - BERT architecture

## üìà Pr√≥ximos Passos
- [ ] Implementar fine-tuning para dom√≠nio espec√≠fico
- [ ] Adicionar suporte para m√∫ltiplos idiomas
- [ ] Criar interface web com Streamlit
- [ ] Implementar avalia√ß√£o autom√°tica de qualidade
- [ ] Adicionar modelos de classifica√ß√£o de texto
- [ ] Integrar com bases de conhecimento

## üîó Projetos Relacionados
- [Go PriceGuard API](../go-priceguard-api/) - Backend com IA features
- [React PriceGuard View](../react-priceguard-view/) - Frontend para IA
- [Java Generation Notes](../java-generation-notes/) - Base de estudos

---

**Desenvolvido por:** Felipe Macedo  
**Contato:** contato.dev.macedo@gmail.com  
**GitHub:** [FelipeMacedo](https://github.com/felipemacedo1)  
**LinkedIn:** [felipemacedo1](https://linkedin.com/in/felipemacedo1)

> üí° **Reflex√£o:** Este projeto abriu minha vis√£o para o potencial da IA na educa√ß√£o. Trabalhar com modelos Transformer e NLP consolidou conhecimentos fundamentais em Machine Learning e suas aplica√ß√µes pr√°ticas.